{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [사전 학습된 모델 테스트]\n",
    "- `https://github.com/BM-K/KoSentenceBERT_SKT`\n",
    "- 삼성전자와 LG화학 관련 뉴스URL 군집 각 2개\n",
    "  - `samsung1`, `samsung2`, `lgchem1`, `lgchem2`\n",
    "- 뉴스 타이틀의 유사성 분석 후 라벨링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [이슈 및 세팅 시 필요한 내용 정리]\n",
    "- 로컬에서 돌렸을 때 경로 충돌 및 다양한 문제 발생\n",
    "  - [참고] `https://github.com/BM-K/KoSentenceBERT_SKT/issues/6`\n",
    "  - 도커허브에 올려주신 이미지로 컨테이너 구성 후 테스트\n",
    "\n",
    "- 컨테이너 내에서 git clone 불가\n",
    "  - [참고] `https://github.com/moby/moby/issues/16600`\n",
    "  - `/etc/docker/daemon.json` 생성 후 dns 세팅 `{\"dns\":[\"8.8.8.8\"]}`\n",
    "\n",
    "- 사전 학습된 PT 파일 복사 (로컬파일을 도커 컨테이너로 전송)\n",
    "  - `docker cp <전송할 파일> <컨테이너명>:<저장할 파일경로>`\n",
    "  - 역 : `docker cp <컨테이너명>:<전송할 파일> <호스트 경로>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 모아둔 URL 리스트\n",
    "import urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 활용, 전처리에 활용\n",
    "# https://www.ranks.nl/stopwords/korean\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords = [\"특파원\", \"현지시간\", \"현지시각\", \"현지\", \"기자\", \"단독\", \"한편\"]\n",
    "    f = open(\"./korean_stopwords.tsv\", \"r\", encoding=\"UTF-8\")\n",
    "    while True:\n",
    "        line = f.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        stopwords.append(line.rstrip())\n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 + 기업이름(keywords) 삭제\n",
    "def preprocessing(url_list, stopwords, keywords, dummy_title=\"각캭콕쿅 눈누날라\"):\n",
    "    # 불용어 + 기업이름 삭제 (전처리)\n",
    "    from newspaper import Article\n",
    "\n",
    "    titles = []\n",
    "    for i, url in enumerate(url_list):\n",
    "        try:\n",
    "            article = Article(url, language=\"ko\")\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            if not article.title or not article.text:\n",
    "                raise (Exception)\n",
    "\n",
    "            words = \"\"\n",
    "            for word in article.title.split():\n",
    "                is_included = True\n",
    "                for keyword in keywords:\n",
    "                    if keyword in word:  # 기업이름\n",
    "                        is_included = False\n",
    "                        break\n",
    "                if not is_included:\n",
    "                    continue\n",
    "                if word not in stopwords:\n",
    "                    words += word + \" \"\n",
    "            titles.append(words.rstrip())\n",
    "\n",
    "        except Exception:\n",
    "            titles.append(dummy_title)\n",
    "            print(f\"{i}번 문서 dummy 값으로 처리\")\n",
    "            continue\n",
    "\n",
    "    return titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코사인 유사도 기반 군집 라벨링\n",
    "def get_cos_sim(corpus):\n",
    "    from KoSentenceBERT_SKTBERT.sentence_transformers import SentenceTransformer, util\n",
    "    import numpy as np\n",
    "\n",
    "    model_path = \"KoSentenceBERT_SKTBERT/output/training_sts\"\n",
    "    embedder = SentenceTransformer(model_path)\n",
    "\n",
    "    embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
    "    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    labels = [i for i in range(len(corpus))]\n",
    "\n",
    "    def find(x):\n",
    "        if labels[x] != x:\n",
    "            labels[x] = find(labels[x])\n",
    "        return labels[x]\n",
    "\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(len(corpus)):\n",
    "            if cos_scores[i, j] > 0.75:\n",
    "                fi, fj = find(i), find(j)\n",
    "                if fi < fj:\n",
    "                    labels[fj] = fi\n",
    "                elif fj < fi:\n",
    "                    labels[fi] = fj\n",
    "    for x in range(len(corpus)):\n",
    "        find(x)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 라벨별 개수 파악\n",
    "def classifier(titles, labels, dummy_title=\"각캭콕쿅 눈누날라\"):\n",
    "    labels = set(labels)\n",
    "    result = []\n",
    "    for idx in labels:\n",
    "        if titles[idx] == dummy_title:\n",
    "            continue\n",
    "        result.append(idx)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    stopwords = get_stopwords()\n",
    "    url_list = urls.lgchem1 + urls.lgchem2\n",
    "    keywords = [\"LG\", \"lg\"]\n",
    "\n",
    "    titles = preprocessing(url_list, stopwords, keywords)\n",
    "    labels = get_cos_sim(titles)\n",
    "    K = len(urls.lgchem1)\n",
    "    \n",
    "    print(\"\\n[라벨링]\")\n",
    "    print(f\"lgchem1 : {labels[:K]}\")\n",
    "    print(f\"lgchem2 : {labels[K:]}\")\n",
    "\n",
    "    print(\"\\n[분류 결과]\")\n",
    "    result = classifier(titles, labels)\n",
    "    print(result)\n",
    "    for idx in result:\n",
    "        print(titles[idx])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
